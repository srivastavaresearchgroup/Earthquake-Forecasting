{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f940525",
   "metadata": {},
   "outputs": [],
   "source": [
    "mseed = 2\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(mseed)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(mseed)\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84df5d2c",
   "metadata": {},
   "source": [
    "# 1. Autoencoder\n",
    "## 1.1 Create Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d23dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeModel(acti, loss, opti, lr):\n",
    "    # Dense AE\n",
    "    inputs = keras.Input(shape=(32,32))\n",
    "\n",
    "    facticvation = 'relu'\n",
    "    \n",
    "    x = keras.layers.Flatten(input_shape=[32, 32])(inputs)\n",
    "    x = keras.layers.Dense(1024, activation=facticvation, name='dense_1024J')(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    x = keras.layers.Dense( 512, activation=facticvation, name='dense_512J')(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    skip_a = x\n",
    "    x = keras.layers.Dense( 256, activation=facticvation, name='dense_256J')(x)\n",
    "    x = keras.layers.Dense( 128, activation=facticvation, name='dense_128J')(x)\n",
    "    x = keras.layers.Dense(  64, activation=facticvation, name='dense_64J')(x)\n",
    "    x = keras.layers.Dense(  32, activation=facticvation, name='dense_32J')(x)\n",
    "    x = keras.layers.Dense(  64, activation=facticvation)(x)\n",
    "    x = keras.layers.Dense( 128, activation=facticvation)(x)\n",
    "    x = keras.layers.Dense( 256, activation=facticvation)(x)\n",
    "    x = keras.layers.Dense( 512, activation=facticvation)(x)\n",
    "    x = keras.layers.add([x, skip_a])\n",
    "    x = keras.layers.Dense(1024, activation=facticvation)(x)\n",
    "       \n",
    "    outputs = keras.layers.Reshape([32, 32])(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='dense_ae_{}_{}_{}_{}'.format(acti, str(keras.optimizers.Adam).split('.')[-1].split(\"'\")[0], lr, loss))\n",
    "    model.compile(loss=[loss, None], optimizer=keras.optimizers.Adam(learning_rate=.00008), metrics=['accuracy'])\n",
    " \n",
    "    return model, 'dense_all_skip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4f398e",
   "metadata": {},
   "source": [
    "## 1.2 Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95401fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ae, subfolder = makeModel('relu', 'mse', keras.optimizers.Adam, .0001)\n",
    "checkpoint_filepath = 'checkpoints_tiles/'+subfolder\n",
    "model_ae.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77d1b9-e926-4339-9c1b-f06488cff95a",
   "metadata": {},
   "source": [
    "# 2 Dilated Recurrent Network \n",
    "## 2.1 Read from the File and Create Slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784b2019-2649-4f78-bff7-041d7c7d5cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeNoEQArray_d14():\n",
    "    f2 = h5py.File('ML_Tiles_000to070_FullBlock.hdf5', 'r')\n",
    "    n_eq_avg = np.array(f2['n_eq'])\n",
    "    maxm_loc = np.array(f2['maxmag'])\n",
    "    f2.close()\n",
    "    \n",
    "    d = 14\n",
    "    eq5 = maxm_loc >= 5.0\n",
    "    neqcrit = n_eq_avg >= 10\n",
    "    eq5mask = np.ones(eq5.shape, dtype=bool)\n",
    "    inx5 = np.argwhere(eq5)\n",
    "    \n",
    "    for ixx, ixy, ixz in inx5:\n",
    "        eq5mask[max(ixx-d+1, 0):ixx+1, max(ixy-16, 0):ixy+16, max(ixz-16, 0):ixz+16] = False\n",
    "    \n",
    "    noeqarray = np.zeros(maxm_loc.shape-np.array([512, 32,32]), dtype=bool)\n",
    "    for i in range(512, maxm_loc.shape[0]):\n",
    "        for j in range(16, maxm_loc.shape[1]-16):\n",
    "            for k in range(16, maxm_loc.shape[2]-16):\n",
    "                if np.max(maxm_loc[i-7:i+7, j-8:j+8, k-8:k+8]) < 4.5:\n",
    "                    if np.mean(n_eq_avg[i-7:i+7, j-8:j+8, k-8:k+8]) > 10:\n",
    "                        if eq5mask[i,j,k]:\n",
    "                            noeqarray[i-512, j-16, k-16] = True\n",
    "        print(i, end='\\r')\n",
    "    return noeqarray\n",
    "    \n",
    "#noeqarray_d14 = makeNoEQArray_d14()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a945967-5fe5-4e2a-b053-bea717f62b8f",
   "metadata": {},
   "source": [
    "# 2.2 Read Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c04e4d-5ea5-4c16-a45f-d6750f86731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = h5py.File('ML_Tiles_000to070_FullBlock.hdf5', 'r')\n",
    "bval_loc = np.array(f2['b_value'])[:, 150:260, 150:260]\n",
    "n_eq_avg = np.array(f2['n_eq'])[:, 150:260, 150:260]\n",
    "maxm_loc = np.array(f2['maxmag'])[:, 150:260, 150:260]\n",
    "dept_avg = np.array(f2['depth'])[:, 150:260, 150:260]\n",
    "noeqarray = np.array(f2['NoEQArray'])[:, 150:228, 150:228] #was 150:222\n",
    "f2.close()\n",
    "\n",
    "b_block = np.clip(np.nan_to_num(bval_loc, posinf=2, neginf=0),0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747950eb-0297-4a5e-9880-57195099ed09",
   "metadata": {},
   "source": [
    "## 2.3 CDN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473ea853-a4d5-4900-83b0-499c847ec8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCDN(size, lr=0.0001, lrschedule=None):\n",
    "    alpha = 0.1\n",
    "    class CDBlock(tf.keras.layers.Layer):\n",
    "        def __init__(self, d1, d2, call, reduce, channels, regularizer=None, **kwargs):\n",
    "            super(CDBlock, self).__init__(**kwargs)\n",
    "\n",
    "            self.dilation = 2**(call-1)\n",
    "\n",
    "            self.convdil_layer = [\n",
    "                tf.keras.layers.Conv3D(filters=channels, kernel_size=(1,2,2), padding='same', strides=(1,2,2)),\n",
    "                tf.keras.layers.Reshape([d1-self.dilation+1, -1]),\n",
    "                tf.keras.layers.Conv1D(filters=(d2**2//channels), kernel_size=2, padding='valid', dilation_rate=self.dilation),\n",
    "                tf.keras.layers.Reshape([d1+1-2**call, d2//channels, d2//channels, channels]),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.ReLU(negative_slope=alpha),\n",
    "                ]\n",
    "\n",
    "            if not reduce:\n",
    "                self.convdil_layer = self.convdil_layer[1:]\n",
    "\n",
    "        def call(self, X):\n",
    "            X = self.convdil_layer[0](X)\n",
    "            for i in range(1, len(self.convdil_layer)):\n",
    "                X = self.convdil_layer[i](X)\n",
    "\n",
    "            return tf.keras.activations.relu(X, alpha=alpha)\n",
    "\n",
    "    inputs = keras.Input(shape=(512,size))\n",
    "    \n",
    "    # With Dropout\n",
    "    x = tf.keras.layers.Dropout(0.5, input_shape=(inputs.shape))(inputs)\n",
    "    \n",
    "    x = tf.keras.layers.Reshape([512, 32, 32, 1])(x) # Adjust input shape to match the other networks\n",
    "    \n",
    "    x = CDBlock(512, 32, 1, True ,  2)(x)\n",
    "    x = CDBlock(512, 32, 2, False,  2)(x)\n",
    "    x = CDBlock(512, 32, 3, True ,  4)(x)\n",
    "    x = CDBlock(512, 32, 4, False,  4)(x)\n",
    "    x = CDBlock(512, 32, 5, True ,  8)(x)\n",
    "    x = CDBlock(512, 32, 6, False,  8)(x)\n",
    "    x = CDBlock(512, 32, 7, True , 16)(x)\n",
    "    x = CDBlock(512, 32, 8, False, 16)(x)\n",
    "    x = CDBlock(512, 32, 9, True , 32)(x)\n",
    "\n",
    "    x = tf.keras.layers.Reshape([-1])(x) \n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x) \n",
    "\n",
    "    adam = keras.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "    print_lr = adam_print_lr(adam)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='ConvDil_test')\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                optimizer=adam, # was 0.00005\n",
    "                metrics=['accuracy', print_lr])        \n",
    "\n",
    "    # 'acc' is automatically converted based on the loss function tf.keras.metrics.BinaryAccuracy()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f1659a-3ed8-4753-a7b2-3fa0eaf94986",
   "metadata": {},
   "source": [
    "## 2.4 Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b8e571-f25f-4093-8405-8cfa062c5cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainProgressive(model, checkpoint_filepath_base, epochs=20, batch_size=32):\n",
    "    logfile = 'epochLlog'\n",
    "    tend = 0\n",
    "    epochL = 5\n",
    "    inputlen = 512\n",
    "    np.random.seed(42)\n",
    "    lastlen = 0\n",
    "    time0 = time.time()\n",
    "    \n",
    "\n",
    "    f2 = h5py.File('ML_Tiles_000to070_FullBlock.hdf5', 'r')\n",
    "    noeqarray = np.array(f2['NoEQArray_d14'])[:, 150:228, 150:228]\n",
    "    f2.close()    \n",
    "    \n",
    "    steps_counter = 0\n",
    "    with open(checkpoint_filepath_base+logfile, 'w'):\n",
    "        print('Logfile created!')\n",
    "    \n",
    "    while tend+30 < bval_loc.shape[0]:\n",
    "        tend = inputlen+(epochL+1)*30\n",
    "        tstart = inputlen+(epochL)*30\n",
    "        \n",
    "        inxs_save = {\n",
    "            'TP': [],\n",
    "            'TN': [],\n",
    "            'VP': [],\n",
    "            'VN': [],\n",
    "        }\n",
    "        \n",
    "        # TRAINING DATA\n",
    "        inxs1 = np.argwhere(maxm_loc[inputlen:tend, 16:-16, 16:-16] >= 5.0) # target = 1\n",
    "\n",
    "        lenEL = inxs1.shape[0]\n",
    "        \n",
    "        inxs0 = np.argwhere(noeqarray[:tend-inputlen])\n",
    "        inxs0 = inxs0[np.random.choice(inxs0.shape[0], size=lenEL)]  \n",
    "        \n",
    "        targets_train = np.append(np.zeros(lenEL, dtype=bool), np.ones(lenEL, dtype='bool'))[::-1]\n",
    "        \n",
    "        data_train = np.append(\n",
    "            np.array([b_block[inx[0]:inx[0]+inputlen, inx[1]:inx[1]+32, inx[2]:inx[2]+32] for inx in inxs1]),\n",
    "            np.array([b_block[inx[0]:inx[0]+inputlen, inx[1]:inx[1]+32, inx[2]:inx[2]+32] for inx in inxs0]),\n",
    "            axis=0\n",
    "        )\n",
    "        \n",
    "        inxs_save['TP'] = inxs1\n",
    "        inxs_save['TN'] = inxs0\n",
    "            \n",
    "        sample_weight = np.ones(targets_train.shape)\n",
    "        if lastlen:\n",
    "            sample_weight[lenEL-lastlen:lenEL]     = targets_train.shape[0]/(lastlen+1)\n",
    "            sample_weight[2*lenEL-lastlen:2*lenEL] = targets_train.shape[0]/(lastlen+1)\n",
    "            print('Weight factor: {}'.format(targets_train.shape[0]/(lastlen+1)))\n",
    "        \n",
    "        # VALIDATION DATA\n",
    "        inxs1 = np.argwhere(maxm_loc[tend:tend+30, 16:-16, 16:-16] >= 5.0) # target = 1\n",
    "        \n",
    "        lenEL = inxs1.shape[0]\n",
    "        \n",
    "        inxs0 = np.argwhere(noeqarray[tend-inputlen:tend-inputlen+30])\n",
    "        inxs0 = inxs0[np.random.choice(inxs0.shape[0], size=lenEL)]\n",
    "        \n",
    "    \n",
    "        if lenEL == 0:\n",
    "            print('No reasonable data in Epoch {}. Skipping'.format(epochL))\n",
    "            epochL += 1\n",
    "            lastlen = False\n",
    "            continue\n",
    "        else:\n",
    "            targets_valid = np.append(np.zeros(lenEL, dtype=bool), np.ones(lenEL, dtype='bool'))[::-1]\n",
    "            \n",
    "\n",
    "            data_valid = np.append(\n",
    "                np.array([b_block[inx[0]+tend-inputlen:inx[0]+tend, inx[1]:inx[1]+32, inx[2]:inx[2]+32] for inx in inxs1]),\n",
    "                np.array([b_block[inx[0]+tend-inputlen:inx[0]+tend, inx[1]:inx[1]+32, inx[2]:inx[2]+32] for inx in inxs0]),\n",
    "                axis=0\n",
    "            )\n",
    "            \n",
    "            lastlen = targets_valid.shape[0]\n",
    "            \n",
    "            inxs_save['VP'] = inxs1\n",
    "            inxs_save['VN'] = inxs0\n",
    "   \n",
    "        # Use AE on data here\n",
    "    \n",
    "        shape_valid = data_valid.shape\n",
    "        shape_train = data_train.shape    \n",
    "      \n",
    "        data_valid = data_valid - np.reshape(model_ae.predict(np.reshape(data_valid, (-1, 32, 32))), (shape_valid))\n",
    "        data_train = data_train - np.reshape(model_ae.predict(np.reshape(data_train, (-1, 32, 32))), (shape_train))\n",
    "\n",
    "        \n",
    "        print('Data Shapes for Epoch* {}:'.format(epochL+1))\n",
    "        print(' Training Data:      {}'.format(data_train.shape))\n",
    "        print(' Validation Data:      {}'.format(data_valid.shape))\n",
    "        \n",
    "        with open(checkpoint_filepath_base+logfile, 'a') as flog:\n",
    "            flog.write('{:4d}, {:4d}, {:4d}, {:6.1f}s\\n'.format(\n",
    "                epochL,\n",
    "                targets_train.shape[0], \n",
    "                targets_valid.shape[0], \n",
    "                #model.optimizer.lr.get_LR(),\n",
    "                time.time()-time0))\n",
    "            time0 = time.time()\n",
    "        \n",
    "        checkpoint_filepath = checkpoint_filepath_base + 'epochL_{:04d}/'.format(epochL)\n",
    "       \n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            save_weights_only=True,\n",
    "            monitor='accuracy',\n",
    "            mode='max',\n",
    "            save_best_only=True)\n",
    "                \n",
    "        history = model.fit(\n",
    "            data_train,\n",
    "            targets_train, \n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            #steps_per_epoch=64,\n",
    "            verbose=2,\n",
    "            sample_weight=sample_weight,\n",
    "            shuffle=True,\n",
    "            validation_data=(data_valid, targets_valid),\n",
    "            callbacks=model_checkpoint_callback) # round brackets very important!\n",
    "        \n",
    "        steps_counter += epochs\n",
    "        \n",
    "        model.load_weights(checkpoint_filepath)\n",
    "        \n",
    "        with open(checkpoint_filepath+'history.pkl', 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "        \n",
    "        with open(checkpoint_filepath+'inxs.pkl', 'wb') as f: #THIS WAS WRONG WHILE USING THE AE\n",
    "            inxs_save['VP'] = np.insert(inxs_save['VP'].astype(float), 3, model.predict(data_valid[len(data_valid)//2:]), axis=1)\n",
    "            inxs_save['VN'] = np.insert(inxs_save['VN'].astype(float), 3, model.predict(data_valid[:len(data_valid)//2]), axis=1)\n",
    "            inxs_save['TP'] = np.insert(inxs_save['TP'].astype(float), 3, model.predict(data_train[len(data_train)//2:]), axis=1)\n",
    "            inxs_save['TN'] = np.insert(inxs_save['TN'].astype(float), 3, model.predict(data_train[:len(data_train)//2]), axis=1)\n",
    "            pickle.dump(inxs_save, f)\n",
    "        \n",
    "        with open(checkpoint_filepath+'val_data.dat', 'w') as f:\n",
    "            f.write('NEQ, maxm, depth, inxs\\n')\n",
    "            for inx in inxs1:\n",
    "                f.write('{}, {}, {}, {} {} {}\\n'.format(\n",
    "                n_eq_avg[tend+inx[0], inx[1]+16, inx[2]+16],\n",
    "                maxm_loc[tend+inx[0], inx[1]+16, inx[2]+16],\n",
    "                dept_avg[tend+inx[0], inx[1]+16, inx[2]+16],\n",
    "                *inx))\n",
    "            for inx in inxs0:\n",
    "                f.write('{} {} {}\\n'.format(*inx))\n",
    "    \n",
    "        print('Meta Training on Epoch {} ended. \\nEnddate: {}\\n\\n\\n'.format(epochL+1, np.datetime64('2000-01-01')+np.timedelta64(tend, 'D')))\n",
    "        epochL += 1\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d2ba1a-c01c-4916-b81c-c5873b538ee7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath_base = '/home/srivastava-shared/data/Japan_progressive/CDN_seeds/s_{:02d}/'.format(mseed)\n",
    "model = makeCDN_aug(32, lr=2e-5, lrschedule=True, aug_shift=2)\n",
    "trainProgressive(model, checkpoint_filepath_base, epochs=20, batch_size=32, aug_shift=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
