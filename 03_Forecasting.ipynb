{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f940525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jkoehler/ENVS/forecasting_venv/lib/python3.8/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "mseed = 2\n",
    "import numpy as np\n",
    "np.random.seed(mseed)\n",
    "import torch\n",
    "torch.manual_seed(mseed)\n",
    "\n",
    "import time\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torcheval.metrics\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import datetime as d\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "masterdir = '/home/seismoai/data/forecasting/'\n",
    "blockdir = masterdir + 'blockdata/'\n",
    "modeldir = masterdir + 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca5268b-b578-4347-9b47-c56e86e10dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU support currently\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Cuda Available')\n",
    "else:\n",
    "    print('No GPU support currently')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84df5d2c",
   "metadata": {},
   "source": [
    "# 1. Autoencoder\n",
    "## 1.1 Create Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdd5b22a-1869-4c4f-a052-576ba80b3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeAE_PT():\n",
    "    class Autoencoder(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv0 = nn.Conv2d(in_channels= 1, out_channels=  4, kernel_size=4)\n",
    "            self.conv1 = nn.Conv2d(in_channels= 4, out_channels=  8, kernel_size=4)\n",
    "            self.conv2 = nn.Conv2d(in_channels= 8, out_channels= 16, kernel_size=4)\n",
    "            self.conv3 = nn.Conv2d(in_channels=16, out_channels= 32, kernel_size=4)\n",
    "            \n",
    "            self.tconv3 = nn.ConvTranspose2d(in_channels=32,   out_channels= 16,  kernel_size=4)\n",
    "            self.tconv2 = nn.ConvTranspose2d(in_channels=16,   out_channels=  8,  kernel_size=4)\n",
    "            self.tconv1 = nn.ConvTranspose2d(in_channels= 8,   out_channels=  4,  kernel_size=4)\n",
    "            self.tconv0 = nn.ConvTranspose2d(in_channels= 4,   out_channels=  1,  kernel_size=4)\n",
    "            \n",
    "            self.acti = nn.functional.relu\n",
    "\n",
    "        def forward(self, x):\n",
    "            y = self.acti(self.conv0(x.view(-1, 1, 32, 32)))\n",
    "            x = self.acti(self.conv1(y))\n",
    "            z = self.acti(self.conv2(x))\n",
    "            x = self.acti(self.conv3(z))\n",
    "            \n",
    "            x = self.acti(self.tconv3(x))\n",
    "            x = self.acti(self.tconv2(x+z))\n",
    "            x = self.acti(self.tconv1(x))\n",
    "            x = self.acti(self.tconv0(x+y))\n",
    "            \n",
    "            return self.acti(x).view(-1, 32, 32)\n",
    "    \n",
    "    return Autoencoder().to(device)\n",
    "\n",
    "def getAE(filename):\n",
    "    model_ae = makeAE_PT()\n",
    "    model_ae.load_state_dict(torch.load(modeldir + filename))\n",
    "    model_ae.eval()\n",
    "    return model_ae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77d1b9-e926-4339-9c1b-f06488cff95a",
   "metadata": {},
   "source": [
    "# 2 Dilated Recurrent Network \n",
    "## 2.1 Read Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98c04e4d-5ea5-4c16-a45f-d6750f86731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = h5py.File(blockdir + 'Tiles_Parameters_0.1.hdf5', 'r')\n",
    "bval_loc = np.array(f2['b_value_730_0.15'])[:, 150:260, 150:260]\n",
    "n_eq_avg = np.array(f2['n_eq_730_0.15'])[:, 150:260, 150:260]\n",
    "maxm_loc = np.array(f2['maxmag'])[:, 150:260, 150:260]\n",
    "dept_avg = np.array(f2['depth'])[:, 150:260, 150:260]\n",
    "noeqarray = np.array(f2['NoEQArray_d730_r0.15_m4.0'])[:, 150:228, 150:228] \n",
    "f2.close()\n",
    "\n",
    "b_block = np.clip(np.nan_to_num(bval_loc, posinf=2, neginf=0),0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747950eb-0297-4a5e-9880-57195099ed09",
   "metadata": {},
   "source": [
    "## 2.2 CDN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f8d1da4-a830-494e-92d3-c22ce3115ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCDN_PT():\n",
    "    class CDBlock(nn.Module):\n",
    "        def __init__(self, call, reduce, channels_in, channels_out):\n",
    "            super(CDBlock, self).__init__()\n",
    "            \n",
    "            self.dilation = 2**(call-1)\n",
    "            self.c_in = channels_in\n",
    "            self.c_out = channels_out\n",
    "            self.reduce = reduce\n",
    "            \n",
    "            self.conv3d_1 = nn.Conv3d(in_channels=self.c_in,  out_channels=self.c_out, kernel_size=(1,2,2), padding=0, stride=(1,2,2))\n",
    "            self.conv3d_2 = nn.Conv3d(in_channels=self.c_out, out_channels=self.c_out, kernel_size=(2,1,1), padding='valid', dilation=(self.dilation,1,1))\n",
    "\n",
    "            self.batch_norm = nn.BatchNorm3d(self.c_out)\n",
    "            self.acti = nn.functional.leaky_relu\n",
    "        \n",
    "        def forward(self, x):\n",
    "            if self.reduce:\n",
    "                x = self.conv3d_1(x)\n",
    "            x = self.conv3d_2(x)\n",
    "            x = self.acti(self.batch_norm(x))\n",
    "            return x\n",
    "\n",
    "    class CDNModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.linear = nn.Linear(in_features=32, out_features=1)\n",
    "            self.sigmoid =  nn.functional.sigmoid\n",
    "\n",
    "            self.blocks = nn.ModuleList([\n",
    "                    CDBlock(1, True ,  1,  2).to(device) ,\n",
    "                    CDBlock(2, False,  2,  2).to(device) ,\n",
    "                    CDBlock(3, True ,  2,  4).to(device) ,\n",
    "                    CDBlock(4, False,  4,  4).to(device) ,\n",
    "                    CDBlock(5, True ,  4,  8).to(device) ,\n",
    "                    CDBlock(6, False,  8,  8).to(device) ,\n",
    "                    CDBlock(7, True ,  8, 16).to(device) ,\n",
    "                    CDBlock(8, False, 16, 16).to(device) ,\n",
    "                    CDBlock(9, True , 16, 32).to(device) ,\n",
    "            ])                \n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x[:, None, :,:,:]\n",
    "            for i, block in enumerate(self.blocks):\n",
    "                x = block(x)\n",
    "            x = self.linear(x.view((-1, 32)))\n",
    "            return torch.squeeze(self.sigmoid(x))\n",
    "            \n",
    "    return CDNModel().to(device)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f1659a-3ed8-4753-a7b2-3fa0eaf94986",
   "metadata": {},
   "source": [
    "## 2.3 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a44f8d66-00b0-4241-83d0-804def9be27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencode(data):\n",
    "    in_shape = data.shape\n",
    "    return  data - model_ae(data).view(in_shape)\n",
    "\n",
    "class DatasetTrain(Dataset):\n",
    "    def __init__(self, meta_epoch, inputlen=512, epochlen=30, minmag=5.0, lastlen=0, use_autoencoder=True):\n",
    "        super(Dataset, self).__init__()\n",
    "\n",
    "        tend = inputlen+(meta_epoch+1)*epochlen\n",
    "\n",
    "        inxs1 = np.argwhere(maxm_loc[inputlen:tend, 16:-16, 16:-16] >= minmag) # target = 1\n",
    "        lenEL = inxs1.shape[0]\n",
    "        \n",
    "        inxs0 = np.argwhere(noeqarray[:tend-inputlen])\n",
    "        inxs0 = inxs0[np.random.choice(inxs0.shape[0], size=lenEL)]  \n",
    "        \n",
    "        data_train = np.append(\n",
    "            np.array([b_block[inx[0]:inx[0]+inputlen, inx[1]:inx[1]+32, inx[2]:inx[2]+32] for inx in inxs1]),\n",
    "            np.array([b_block[inx[0]:inx[0]+inputlen, inx[1]:inx[1]+32, inx[2]:inx[2]+32] for inx in inxs0]),\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        self.inxs_save = {'TP': inxs1, 'TN': inxs0}\n",
    "        self.targets = torch.cat((torch.ones(lenEL), torch.zeros(lenEL))).to(bool)\n",
    "\n",
    "        sample_weight = np.ones(self.targets.shape)\n",
    "        if lastlen:\n",
    "            sample_weight[lenEL-lastlen:lenEL]     = targets_train.shape[0]/(lastlen+1)\n",
    "            sample_weight[2*lenEL-lastlen:2*lenEL] = targets_train.shape[0]/(lastlen+1)\n",
    "            print('Weight factor: {}'.format(targets_train.shape[0]/(lastlen+1)))\n",
    "\n",
    "        self.weight = torch.from_numpy(sample_weight)\n",
    "        self.weight /= self.weight.max()\n",
    "\n",
    "        data_train = torch.from_numpy(data_train).to(torch.float32).to(device)\n",
    "\n",
    "        if use_autoencoder:\n",
    "            self.data = autoencode(data_train)\n",
    "        else:\n",
    "            self.data = data_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.targets[index], self.weight[index]\n",
    "\n",
    "class DatasetValid(Dataset):\n",
    "    def __init__(self, meta_epoch, inputlen=512, epochlen=30, minmag=5.0, lastlen=0, use_autoencoder=True):\n",
    "        super(Dataset, self).__init__()\n",
    "        \n",
    "        tend = inputlen+(meta_epoch+1)*epochlen\n",
    "        \n",
    "        inxs1 = np.argwhere(maxm_loc[tend:tend+epochlen, 16:-16, 16:-16] >= minmag) # target = 1\n",
    "        lenEL = inxs1.shape[0]\n",
    "        \n",
    "        if lenEL == 0:\n",
    "            self.is_empty = True \n",
    "        else: \n",
    "            self.is_empty =  False\n",
    "            self.lastlen = lenEL\n",
    "            \n",
    "            inxs0 = np.argwhere(noeqarray[tend-inputlen:tend-inputlen+epochlen])\n",
    "            inxs0 = inxs0[np.random.choice(inxs0.shape[0], size=lenEL)]\n",
    "\n",
    "            data_valid = np.append(\n",
    "                np.array([b_block[inx[0]+tend-inputlen:inx[0]+tend, inx[1]:inx[1]+32, inx[2]:inx[2]+32] for inx in inxs1]),\n",
    "                np.array([b_block[inx[0]+tend-inputlen:inx[0]+tend, inx[1]:inx[1]+32, inx[2]:inx[2]+32] for inx in inxs0]),\n",
    "                axis=0\n",
    "            )\n",
    "                    \n",
    "            self.inxs_save = {'VP': inxs1, 'VN': inxs0}\n",
    "\n",
    "            data_valid = torch.from_numpy(data_valid).to(torch.float32).to(device)\n",
    "            if use_autoencoder:\n",
    "                self.data = autoencode(data_valid)\n",
    "            else:\n",
    "                self.data = data_valid\n",
    "                \n",
    "            self.targets = torch.cat((torch.ones(lenEL), torch.zeros(lenEL))).to(bool)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.targets[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0e6630-d881-4864-8dad-ce66f114359f",
   "metadata": {},
   "source": [
    "## 2.4 Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9895a3c3-ce49-471f-9abf-edc4a56cc003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataloaders(meta_epoch, lastlen=False, inputlen=512, epochlen=30, minmag=5.0, use_autoencoder=True, batch_size=16):\n",
    "    time0 = time.time()\n",
    "    \n",
    "    while True:\n",
    "        validset = DatasetValid(meta_epoch, inputlen, epochlen, minmag, lastlen, use_autoencoder)\n",
    "        if validset.is_empty:\n",
    "            lastlen = False\n",
    "            print('Meta Epoch {} contains no events >= {} M. Skipping!'.format(meta_epoch, minmag))\n",
    "            meta_epoch += 1\n",
    "            if inputlen+(meta_epoch+1)*epochlen > maxm_loc.shape[0]:\n",
    "                print('inputlen+(meta_epoch+1)*epochlen > maxm_loc.shape[0]')\n",
    "                print('Triggered in getDataloaders')\n",
    "                print(inputlen, meta_epoch, epochlen, maxm_loc.shape[0])   \n",
    "                return False\n",
    "        else:\n",
    "            lastlen_new = validset.lastlen\n",
    "            meta_epoch += 1\n",
    "            break\n",
    "            \n",
    "    \n",
    "    trainset = DatasetTrain(meta_epoch-1, inputlen, epochlen, minmag, lastlen, use_autoencoder)\n",
    "    lastlen = lastlen_new\n",
    "    \n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    validloader = DataLoader(validset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    time1 = time.time()\n",
    "    print(f'ME:{meta_epoch-1:>3d}: | Batches ({len(trainloader):>4d}/{len(validloader):>4d}) samples: ({len(trainset):>4d}/{len(validset):>4d}) ', end='')\n",
    "\n",
    "    return trainloader, validloader, (len(trainset), len(validset)), meta_epoch, lastlen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50988a1e-b214-4674-a8f8-e7a0db4ba31f",
   "metadata": {},
   "source": [
    "## 2.5 Define Training Functions\n",
    "train_std: Normal training, to be executed on a model and on a dataset  \n",
    "train_progressive: Progressive training, calls train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff0cd5b7-7ae8-4dcd-a731-73c42679935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_std(model, meta_epoch, epochs, trainloader, validloader, setlens, lossfunction, optimizer, run_name, checkpoint_filepath_base):\n",
    "    loss_train = []\n",
    "    loss_valid = []\n",
    "    earlystopping_counter = 0\n",
    "\n",
    "    targets_list = []\n",
    "    predict_list = []\n",
    "\n",
    "    def reduce_loss(loss, weights=None):\n",
    "        if type(weights) == type(None):\n",
    "            return torch.mean(loss)\n",
    "        else:\n",
    "            return torch.mean(loss*weights)\n",
    "    \n",
    "    \n",
    "    for e in range(epochs):\n",
    "        time0 = time.time()\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for data, targets, weights in trainloader:\n",
    "            data, targets, weights = data.to(device), targets.to(torch.float32).to(device), weights.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(data)\n",
    "\n",
    "            loss = lossfunction(preds, targets)\n",
    "                        \n",
    "            loss = reduce_loss(loss, weights=weights)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        model.eval()\n",
    "        for data, targets in validloader:\n",
    "            data, targets = data.to(device), targets.to(torch.float32).to(device)\n",
    "            preds = model(data)\n",
    "\n",
    "            loss = reduce_loss(lossfunction(preds, targets))\n",
    "            \n",
    "            valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "            if e == epochs-1:\n",
    "                targets_list.extend(targets.detach().tolist())\n",
    "                predict_list.extend(  preds.detach().tolist())\n",
    "        \n",
    "        #scheduler.step()\n",
    "\n",
    "        loss_train.append(train_loss)\n",
    "        loss_valid.append(valid_loss)\n",
    "\n",
    "\n",
    "    savefile = checkpoint_filepath_base + f'classifier_{meta_epoch:03d}.pt'\n",
    "    torch.save(model.state_dict(), savefile)\n",
    "\n",
    "    with open(checkpoint_filepath_base+run_name+f'_metaepoch_{meta_epoch:03d}.pkl', 'wb') as picklefile:\n",
    "        pickle.dump([targets_list, predict_list], picklefile)\n",
    "\n",
    "    preds = np.where(np.array(predict_list)>=0.5, 1, 0)\n",
    "    tgts = np.array(targets_list)\n",
    "    \n",
    "    acc = np.sum(preds == tgts) / tgts.shape[0]\n",
    "    \n",
    "    print(f' | acc = {acc:5.3f} | {time.asctime(time.localtime())[11:19]} ', end='')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def trainProgressive(model, \n",
    "                     checkpoint_filepath_base, \n",
    "                     epochs=20, \n",
    "                     epochlen=30, \n",
    "                     minmag=5.0, \n",
    "                     use_autoencoder=True, \n",
    "                     batch_size=32, \n",
    "                     inputlen=512, \n",
    "                     lossfunction=None, \n",
    "                     optimizer=None, \n",
    "                     run_name='default'):\n",
    "    meta_epoch = 5\n",
    "    lastlen = False\n",
    "    tend = 0\n",
    "    logfile = 'epochLlog'\n",
    "    time0 = time.time()\n",
    "    \n",
    "    while tend+epochlen < bval_loc.shape[0]:\n",
    "        tend = inputlen+(meta_epoch+1)*epochlen\n",
    "        tstart = inputlen+(meta_epoch)*epochlen\n",
    "\n",
    "        # Get Dataloaders\n",
    "        res = getDataloaders(meta_epoch=meta_epoch, epochlen=epochlen, use_autoencoder=False, batch_size=batch_size)\n",
    "        if res:\n",
    "            trainloader, validloader, setlens, meta_epoch, lastlen = res\n",
    "        else:\n",
    "            print('Finished Training')\n",
    "            break\n",
    "\n",
    "        with open(checkpoint_filepath_base+logfile, 'a') as flog:\n",
    "            flog.write('{:4d}, {:4d}, {:4d}, {:6.1f}s\\n'.format(\n",
    "                meta_epoch,\n",
    "                setlens[0], \n",
    "                setlens[1], \n",
    "                time.time()-time0))\n",
    "            time0 = time.time() # reset time per epoch\n",
    "\n",
    "        model = train_std(model, meta_epoch, epochs, trainloader, validloader, setlens, lossfunction, optimizer, run_name, checkpoint_filepath_base)\n",
    "\n",
    "        tend = inputlen+(meta_epoch+1)*epochlen\n",
    "        print(f'| Date {str(d.date(2000,1,1) + d.timedelta(days=tend))}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0168779-a24b-4127-9fee-e285d507b54d",
   "metadata": {},
   "source": [
    "# 3. Call Training Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f34e1a1c-8c4d-4454-875e-9696e8cfe019",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ae = getAE('AE_d365_r0.25_m4.5.pth')\n",
    "\n",
    "model = makeCDN_PT()\n",
    "lossfuction = nn.L1Loss(reduction='none') # L1 Loss by default\n",
    "\n",
    "lr = 1e-4\n",
    "lr_wdecay = lr*1e-1\n",
    "optimizer = torch.optim.Adam(list(model.parameters()), lr=lr, weight_decay=lr_wdecay)\n",
    "\n",
    "epochs = 20 # epochs per meta epoch\n",
    "epochlen = 30 # days per meta epoch\n",
    "batch_size = 32\n",
    "\n",
    "run_name = 'Test_d365_r0.25_m4.5_e{}_ed{}_b{}_bce'.format(epochs, epochlen, batch_size)\n",
    "checkpoint_filepath_base = modeldir + 'checkpoints_'+run_name+'/'\n",
    "\n",
    "if not os.path.exists(checkpoint_filepath_base):\n",
    "    os.makedirs(checkpoint_filepath_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3676b9-e88c-4a63-b70a-d2da84e17b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ME:  5: | Batches (   1/   1) samples: (  26/   6) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jkoehler/ENVS/forecasting_venv/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | acc = 0.500 | 13:39:32 | Date 2001-12-23\n",
      "ME:  6: | Batches (   1/   1) samples: (  28/   2)  | acc = 1.000 | 13:39:34 | Date 2002-01-22\n",
      "ME:  7: | Batches (   2/   1) samples: (  40/  12)  | acc = 0.500 | 13:39:36 | Date 2002-02-21\n",
      "Meta Epoch 8 contains no events >= 5.0 M. Skipping!\n",
      "ME:  9: | Batches (   2/   1) samples: (  42/   2)  | acc = 0.500 | 13:39:38 | Date 2002-04-22\n",
      "ME: 10: | Batches (   2/   1) samples: (  46/   4)  | acc = 0.750 | 13:39:41 | Date 2002-05-22\n",
      "ME: 11: | Batches (   2/   1) samples: (  48/   2)  | acc = 0.500 | 13:39:43 | Date 2002-06-21\n",
      "ME: 12: | Batches (   2/   1) samples: (  50/   2)  | acc = 1.000 | 13:39:45 | Date 2002-07-21\n",
      "ME: 13: | Batches (   2/   1) samples: (  54/   4)  | acc = 1.000 | 13:39:48 | Date 2002-08-20\n",
      "Meta Epoch 14 contains no events >= 5.0 M. Skipping!\n",
      "ME: 15: | Batches (   2/   1) samples: (  62/   8)  | acc = 0.500 | 13:39:51 | Date 2002-10-19\n",
      "ME: 16: | Batches (   3/   1) samples: (  66/   4)  | acc = 0.750 | 13:39:54 | Date 2002-11-18\n",
      "ME: 17: | Batches (   3/   1) samples: (  70/   4)  | acc = 1.000 | 13:39:57 | Date 2002-12-18\n",
      "ME: 18: | Batches (   3/   1) samples: (  74/   4)  | acc = 1.000 | 13:40:01 | Date 2003-01-17\n",
      "ME: 19: | Batches (   3/   1) samples: (  76/   2)  | acc = 0.500 | 13:40:04 | Date 2003-02-16\n",
      "ME: 20: | Batches (   3/   1) samples: (  80/   4)  | acc = 0.750 | 13:40:08 | Date 2003-03-18\n",
      "Meta Epoch 21 contains no events >= 5.0 M. Skipping!\n",
      "ME: 22: | Batches (   3/   1) samples: (  82/   2)  | acc = 1.000 | 13:40:12 | Date 2003-05-17\n",
      "ME: 23: | Batches (   3/   1) samples: (  88/   6)  | acc = 0.833 | 13:40:16 | Date 2003-06-16\n",
      "ME: 24: | Batches (   3/   1) samples: (  90/   2)  | acc = 1.000 | 13:40:20 | Date 2003-07-16\n",
      "ME: 25: | Batches (   3/   1) samples: (  96/   6)  | acc = 0.333 | 13:40:24 | Date 2003-08-15\n",
      "ME: 26: | Batches (   4/   1) samples: (  98/   2)  | acc = 1.000 | 13:40:28 | Date 2003-09-14\n",
      "ME: 27: | Batches (   5/   2) samples: ( 156/  58)  | acc = 0.741 | 13:40:35 | Date 2003-10-14\n",
      "ME: 28: | Batches (   6/   1) samples: ( 174/  18)  | acc = 0.667 | 13:40:43 | Date 2003-11-13\n",
      "ME: 29: | Batches (   6/   1) samples: ( 186/  12)  | acc = 0.750 | 13:40:50 | Date 2003-12-13\n",
      "ME: 30: | Batches (   6/   1) samples: ( 192/   6)  | acc = 0.833 | 13:40:58 | Date 2004-01-12\n",
      "ME: 31: | Batches (   7/   1) samples: ( 198/   6)  | acc = 0.833 | 13:41:06 | Date 2004-02-11\n",
      "ME: 32: | Batches (   7/   1) samples: ( 200/   2)  | acc = 0.500 | 13:41:15 | Date 2004-03-12\n",
      "ME: 33: | Batches (   7/   1) samples: ( 206/   6)  | acc = 1.000 | 13:41:24 | Date 2004-04-11\n",
      "Meta Epoch 34 contains no events >= 5.0 M. Skipping!\n",
      "ME: 35: | Batches (   7/   1) samples: ( 208/   2)  | acc = 1.000 | 13:41:32 | Date 2004-06-10\n",
      "ME: 36: | Batches (   7/   1) samples: ( 218/  10)  | acc = 0.700 | 13:41:41 | Date 2004-07-10\n",
      "ME: 37: | Batches (   7/   1) samples: ( 222/   4)  | acc = 0.750 | 13:41:50 | Date 2004-08-09\n",
      "ME: 38: | Batches (   8/   1) samples: ( 230/   8)  | acc = 0.625 | 13:42:00 | Date 2004-09-08\n",
      "Meta Epoch 39 contains no events >= 5.0 M. Skipping!\n",
      "ME: 40: | Batches (   9/   1) samples: ( 258/  28)  | acc = 0.821 | 13:42:11 | Date 2004-11-07\n",
      "ME: 41: | Batches (   9/   1) samples: ( 278/  20)  | acc = 0.900 | 13:42:22 | Date 2004-12-07\n",
      "ME: 42: | Batches (   9/   1) samples: ( 284/   6)  | acc = 0.667 | 13:42:34 | Date 2005-01-06\n",
      "ME: 43: | Batches (  10/   1) samples: ( 292/   8)  | acc = 1.000 | 13:42:46 | Date 2005-02-05\n",
      "ME: 44: | Batches (  10/   1) samples: ( 296/   4)  | acc = 0.750 | 13:42:58 | Date 2005-03-07\n",
      "ME: 45: | Batches (  10/   1) samples: ( 300/   4)  | acc = 0.750 | 13:43:10 | Date 2005-04-06\n",
      "Meta Epoch 46 contains no events >= 5.0 M. Skipping!\n",
      "ME: 47: | Batches (  10/   1) samples: ( 302/   2)  | acc = 1.000 | 13:43:23 | Date 2005-06-05\n",
      "ME: 48: | Batches (  10/   1) samples: ( 306/   4)  | acc = 0.500 | 13:43:35 | Date 2005-07-05\n",
      "ME: 49: | Batches (  10/   1) samples: ( 310/   4)  | acc = 0.500 | 13:43:48 | Date 2005-08-04\n",
      "ME: 50: | Batches (  11/   1) samples: ( 326/  16)  | acc = 0.875 | 13:44:01 | Date 2005-09-03\n",
      "Meta Epoch 51 contains no events >= 5.0 M. Skipping!\n",
      "ME: 52: | Batches (  11/   1) samples: ( 338/  12)  | acc = 0.750 | 13:44:15 | Date 2005-11-02\n",
      "ME: 53: | Batches (  11/   1) samples: ( 340/   2)  | acc = 0.000 | 13:44:29 | Date 2005-12-02\n",
      "ME: 54: | Batches (  12/   1) samples: ( 356/  16)  | acc = 0.875 | 13:44:44 | Date 2006-01-01\n",
      "ME: 55: | Batches (  12/   1) samples: ( 360/   4)  | acc = 1.000 | 13:44:58 | Date 2006-01-31\n",
      "ME: 56: | Batches (  12/   1) samples: ( 362/   2)  | acc = 1.000 | 13:45:13 | Date 2006-03-02\n",
      "ME: 57: | Batches (  12/   1) samples: ( 364/   2)  | acc = 1.000 | 13:45:28 | Date 2006-04-01\n",
      "ME: 58: | Batches (  12/   1) samples: ( 370/   6)  | acc = 1.000 | 13:45:43 | Date 2006-05-01\n",
      "ME: 59: | Batches (  12/   1) samples: ( 372/   2)  | acc = 1.000 | 13:45:58 | Date 2006-05-31\n",
      "ME: 60: | Batches (  12/   1) samples: ( 376/   4)  | acc = 0.500 | 13:46:13 | Date 2006-06-30\n",
      "ME: 61: | Batches (  12/   1) samples: ( 380/   4)  | acc = 0.750 | 13:46:28 | Date 2006-07-30\n",
      "ME: 62: | Batches (  12/   1) samples: ( 382/   2)  | acc = 1.000 | 13:46:44 | Date 2006-08-29\n",
      "Meta Epoch 63 contains no events >= 5.0 M. Skipping!\n",
      "ME: 64: | Batches (  13/   1) samples: ( 394/  12)  | acc = 1.000 | 13:47:00 | Date 2006-10-28\n",
      "ME: 65: | Batches (  13/   1) samples: ( 396/   2)  | acc = 1.000 | 13:47:16 | Date 2006-11-27\n",
      "ME: 66: | Batches (  13/   1) samples: ( 402/   6)  | acc = 0.667 | 13:47:32 | Date 2006-12-27\n",
      "ME: 67: | Batches (  13/   1) samples: ( 408/   6)  | acc = 1.000 | 13:47:49 | Date 2007-01-26\n",
      "ME: 68: | Batches (  13/   1) samples: ( 412/   4)  | acc = 0.750 | 13:48:05 | Date 2007-02-25\n",
      "ME: 69: | Batches (  14/   1) samples: ( 424/  12)  | acc = 0.917 | 13:48:22 | Date 2007-03-27\n",
      "ME: 70: | Batches (  14/   1) samples: ( 426/   2)  | acc = 1.000 | 13:48:40 | Date 2007-04-26\n",
      "ME: 71: | Batches (  14/   1) samples: ( 432/   6)  | acc = 1.000 | 13:48:57 | Date 2007-05-26\n",
      "ME: 72: | Batches (  14/   1) samples: ( 434/   2)  | acc = 1.000 | 13:49:15 | Date 2007-06-25\n",
      "ME: 73: | Batches (  14/   1) samples: ( 440/   6)  | acc = 1.000 | 13:49:33 | Date 2007-07-25\n",
      "Meta Epoch 74 contains no events >= 5.0 M. Skipping!\n",
      "ME: 75: | Batches (  14/   1) samples: ( 442/   2)  | acc = 0.500 | 13:49:51 | Date 2007-09-23\n",
      "Meta Epoch 76 contains no events >= 5.0 M. Skipping!\n",
      "Meta Epoch 77 contains no events >= 5.0 M. Skipping!\n",
      "ME: 78: | Batches (  14/   1) samples: ( 446/   4)  | acc = 0.750 | 13:50:09 | Date 2007-12-22\n",
      "ME: 79: | Batches (  14/   1) samples: ( 448/   2)  | acc = 1.000 | 13:50:27 | Date 2008-01-21\n",
      "ME: 80: | Batches (  15/   1) samples: ( 454/   6)  | acc = 1.000 | 13:50:45 | Date 2008-02-20\n",
      "Meta Epoch 81 contains no events >= 5.0 M. Skipping!\n",
      "ME: 82: | Batches (  15/   1) samples: ( 456/   2)  | acc = 1.000 | 13:51:04 | Date 2008-04-20\n",
      "ME: 83: | Batches (  15/   1) samples: ( 462/   6)  | acc = 1.000 | 13:51:23 | Date 2008-05-20\n",
      "ME: 84: | Batches (  16/   1) samples: ( 484/  22)  | acc = 0.682 | 13:51:44 | Date 2008-06-19\n",
      "ME: 85: | Batches (  16/   1) samples: ( 490/   6)  | acc = 0.667 | 13:52:03 | Date 2008-07-19\n",
      "ME: 86: | Batches (  16/   1) samples: ( 504/  14)  | acc = 0.929 | 13:52:24 | Date 2008-08-18\n",
      "ME: 87: | Batches (  17/   1) samples: ( 516/  12)  | acc = 0.833 | 13:52:45 | Date 2008-09-17\n",
      "ME: 88: | Batches (  17/   1) samples: ( 522/   6)  | acc = 1.000 | 13:53:07 | Date 2008-10-17\n",
      "Meta Epoch 89 contains no events >= 5.0 M. Skipping!\n",
      "ME: 90: | Batches (  17/   1) samples: ( 528/   6)  | acc = 1.000 | 13:53:28 | Date 2008-12-16\n",
      "ME: 91: | Batches (  17/   1) samples: ( 538/  10)  | acc = 1.000 | 13:53:50 | Date 2009-01-15\n",
      "ME: 92: | Batches (  17/   1) samples: ( 542/   4)  | acc = 1.000 | 13:54:12 | Date 2009-02-14\n",
      "ME: 93: | Batches (  18/   1) samples: ( 552/  10)  | acc = 0.700 | 13:54:34 | Date 2009-03-16\n",
      "Meta Epoch 94 contains no events >= 5.0 M. Skipping!\n",
      "ME: 95: | Batches (  18/   1) samples: ( 554/   2)  | acc = 1.000 | 13:54:57 | Date 2009-05-15\n",
      "ME: 96: | Batches (  18/   1) samples: ( 560/   6)  | acc = 1.000 | 13:55:20 | Date 2009-06-14\n",
      "ME: 97: | Batches (  18/   1) samples: ( 568/   8)  | acc = 0.875 | 13:55:42 | Date 2009-07-14\n",
      "ME: 98: | Batches (  18/   1) samples: ( 572/   4)  | acc = 1.000 | 13:56:06 | Date 2009-08-13\n",
      "ME: 99: | Batches (  18/   1) samples: ( 576/   4)  | acc = 0.750 | 13:56:29 | Date 2009-09-12\n",
      "ME:100: | Batches (  19/   1) samples: ( 580/   4)  | acc = 0.750 | 13:56:53 | Date 2009-10-12\n",
      "ME:101: | Batches (  19/   1) samples: ( 584/   4)  | acc = 1.000 | 13:57:17 | Date 2009-11-11\n",
      "ME:102: | Batches (  19/   1) samples: ( 586/   2)  | acc = 0.500 | 13:57:41 | Date 2009-12-11\n",
      "ME:103: | Batches (  19/   1) samples: ( 592/   6)  | acc = 0.833 | 13:58:04 | Date 2010-01-10\n",
      "ME:104: | Batches (  19/   1) samples: ( 596/   4)  | acc = 0.750 | 13:58:29 | Date 2010-02-09\n",
      "ME:105: | Batches (  19/   1) samples: ( 598/   2)  | acc = 1.000 | 13:58:53 | Date 2010-03-11\n",
      "ME:106: | Batches (  19/   1) samples: ( 602/   4)  | acc = 1.000 | 13:59:17 | Date 2010-04-10\n",
      "Meta Epoch 107 contains no events >= 5.0 M. Skipping!\n",
      "ME:108: | Batches (  19/   1) samples: ( 604/   2)  | acc = 1.000 | 13:59:42 | Date 2010-06-09\n",
      "ME:109: | Batches (  20/   1) samples: ( 614/  10)  | acc = 0.800 | 14:00:07 | Date 2010-07-09\n",
      "ME:110: | Batches (  20/   1) samples: ( 618/   4)  | acc = 0.500 | 14:00:33 | Date 2010-08-08\n",
      "ME:111: | Batches (  20/   1) samples: ( 628/  10)  | acc = 0.900 | 14:00:58 | Date 2010-09-07\n",
      "ME:112: | Batches (  20/   1) samples: ( 636/   8)  | acc = 0.875 | 14:01:24 | Date 2010-10-07\n",
      "ME:113: | Batches (  20/   1) samples: ( 640/   4)  | acc = 1.000 | 14:01:50 | Date 2010-11-06\n",
      "Meta Epoch 114 contains no events >= 5.0 M. Skipping!\n",
      "ME:115: | Batches (  21/   1) samples: ( 650/  10)  | acc = 0.900 | 14:02:17 | Date 2011-01-05\n",
      "Meta Epoch 116 contains no events >= 5.0 M. Skipping!\n",
      "ME:117: | Batches (  21/   1) samples: ( 666/  16)  | acc = 1.000 | 14:02:44 | Date 2011-03-06\n",
      "ME:118: | Batches (  56/  35) samples: (1776/1110)  | acc = 0.940 | 14:04:05 | Date 2011-04-05\n",
      "ME:119: | Batches (  60/   5) samples: (1916/ 140)  | acc = 0.950 | 14:05:20 | Date 2011-05-05\n",
      "ME:120: | Batches (  62/   2) samples: (1980/  64)  | acc = 0.969 | 14:06:36 | Date 2011-06-04\n",
      "ME:121: | Batches (  64/   2) samples: (2024/  44)  | acc = 0.932 | 14:07:54 | Date 2011-07-04\n",
      "ME:122: | Batches (  66/   2) samples: (2088/  64)  | acc = 0.953 | 14:09:14 | Date 2011-08-03\n",
      "ME:123: | Batches (  67/   1) samples: (2118/  30)  | acc = 0.933 | 14:10:35 | Date 2011-09-02\n",
      "ME:124: | Batches (  68/   2) samples: (2164/  46)  | acc = 0.957 | 14:11:58 | Date 2011-10-02\n",
      "ME:125: | Batches (  69/   1) samples: (2182/  18)  | acc = 0.944 | 14:13:20 | Date 2011-11-01\n",
      "ME:126: | Batches (  69/   1) samples: (2192/  10)  | acc = 1.000 | 14:14:44 | Date 2011-12-01\n"
     ]
    }
   ],
   "source": [
    "trainProgressive(model, \n",
    "                 checkpoint_filepath_base, \n",
    "                 epochs=epochs, \n",
    "                 epochlen=epochlen, \n",
    "                 minmag=5.0, \n",
    "                 use_autoencoder=False,\n",
    "                 batch_size=batch_size, \n",
    "                 inputlen=512,\n",
    "                 optimizer=optimizer,\n",
    "                 lossfunction=lossfuction,\n",
    "                 run_name=run_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
